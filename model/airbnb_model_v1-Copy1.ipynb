{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(folder):\n",
    "    #Listing entries present in given folder\n",
    "    entries = os.listdir(folder)\n",
    "    for i in entries:\n",
    "        if 'csv' not in i:\n",
    "            entries.remove(i)\n",
    "    return sorted(entries, reverse=True)[0:12]\n",
    "\n",
    "#Function for normalizing data \n",
    "def nomarlize_price(my_dataframe):\n",
    "    my_dataframe['price'] = (my_dataframe['price'] - \n",
    "                             my_dataframe['price'].min())/(my_dataframe['price'].max() - \n",
    "                                                         my_dataframe['price'].min())\n",
    "    return my_dataframe\n",
    "\n",
    "\n",
    "train_path = \"../split_datasets/train/\"\n",
    "test_path = \"../split_datasets/test/\"\n",
    "val_path = \"../split_datasets/validation/\"\n",
    "\n",
    "train_files = get_file_names(train_path)\n",
    "test_files = get_file_names(test_path)\n",
    "val_files = get_file_names(val_path)\n",
    "\n",
    "frames = []\n",
    "for i in train_files:\n",
    "    frames.append(pd.read_csv(train_path+i))\n",
    "train_df = pd.concat(frames, sort = True)\n",
    "train_df = train_df.drop(columns = 'id')\n",
    "train_df = train_df.drop(columns = 'last_scraped')\n",
    "train_df = train_df.fillna(0)\n",
    "train_df = nomarlize_price(train_df)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "frames = []\n",
    "for i in test_files:\n",
    "    frames.append(pd.read_csv(test_path+i))\n",
    "test_df = pd.concat(frames, sort = True)\n",
    "test_df = test_df.drop(columns = 'id')\n",
    "test_df = test_df.drop(columns = 'last_scraped')\n",
    "test_df = test_df.fillna(0)\n",
    "test_df = nomarlize_price(test_df)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "frames = []\n",
    "for i in val_files:\n",
    "    frames.append(pd.read_csv(val_path+i))\n",
    "val_df = pd.concat(frames, sort = True)\n",
    "val_df = val_df.drop(columns = 'id')\n",
    "val_df = val_df.drop(columns = 'last_scraped')\n",
    "val_df = val_df.fillna(0)\n",
    "val_df = nomarlize_price(val_df)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class airbnb_dataset (Dataset):\n",
    "    def __init__(self, df, purpose):\n",
    "        self.df = df\n",
    "        self.price = self.df[\"price\"]\n",
    "        self.df = self.df.drop(columns=\"price\")\n",
    "        self.purpose = purpose\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Returns specific sample as a dict\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        sample = torch.tensor(self.df.iloc[index]).float()\n",
    "        label = torch.tensor([self.price.iloc[index]]).float()\n",
    "        return {\"sample\": sample, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = airbnb_dataset(train_df, \"train\")\n",
    "test_ds = airbnb_dataset(test_df, \"test\")\n",
    "val_ds = airbnb_dataset(val_df, \"test\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1000, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=1000, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=1000, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class airbnb_net (nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(airbnb_net, self).__init__()\n",
    "        self.layer1 = nn.Linear(264, 100)\n",
    "        self.layer2 = nn.Linear(100, 50)\n",
    "        self.layer3 = nn.Linear(50, 15)\n",
    "        self.layer4 = nn.Linear(15, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        return self.layer4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    plt.title(\"Training Curve\")\\n    plt.plot(epochs, train_acc, label=\"Train\")\\n    plt.plot(epochs, valid_acc, label=\"Validation\")\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Accuracy\")\\n    plt.legend(loc=\\'best\\')\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(net, train_loader, val_loader):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum = 0.9)\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    start_time = time.time()\n",
    "    epochs, losses = [], []\n",
    "    \n",
    "    for epoch in range(75):\n",
    "        epoch_loss = 0\n",
    "        epoch_time = time.time()\n",
    "        batch_loss = 0\n",
    "        batch_time = time.time()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            sample = data[\"sample\"]\n",
    "            label = data[\"label\"]\n",
    "            output = net(sample)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss += loss\n",
    "            epoch_loss += loss\n",
    "            break\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                batch_loss = batch_loss/10\n",
    "                print(\"---[ITER %d] loss: %.3f  time: %.3f\" % (i, batch_loss, time.time()-batch_time))\n",
    "                batch_loss = 0\n",
    "                batch_time = time.time()\n",
    "        epoch_loss /= len(train_loader)\n",
    "        \n",
    "        epochs.append(epoch)\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        print(\"[EPOCH %d] loss: %.3f  time: %.3f\" % (epoch+1, epoch_loss, time.time()-epoch_time))\n",
    "    \n",
    "    print(\"=========================================\")\n",
    "    print(\"Training Completed...\")\n",
    "    print(\"[FINAL] loss: %.3f  time: %.3f\" % (epoch_loss, time.time()-start_time))\n",
    "          \n",
    "        \n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "'''\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "'''            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(model, train_loader, val_loader):\n",
    "    train_samples = 0\n",
    "    train_error = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        sample = data[\"sample\"]\n",
    "        label = data[\"label\"]\n",
    "        output = net(sample)\n",
    "        error = abs(label-output)\n",
    "        accurate = (error < 10)\n",
    "        return accurate[0:10].sum()\n",
    "        train_samples += len(label)\n",
    "        train_error += error.sum().item()\n",
    "    val_samples = 0\n",
    "    val_error = 0\n",
    "    for i, data in enumerate(val_loader):\n",
    "        print(\"hello\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = airbnb_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "[EPOCH 1] loss: 0.007  time: 17.014\n",
      "[EPOCH 2] loss: 0.003  time: 18.342\n",
      "[EPOCH 3] loss: 0.007  time: 16.629\n",
      "[EPOCH 4] loss: 0.001  time: 12.986\n",
      "[EPOCH 5] loss: 0.006  time: 13.060\n",
      "[EPOCH 6] loss: 0.003  time: 11.519\n",
      "[EPOCH 7] loss: 0.001  time: 11.981\n",
      "[EPOCH 8] loss: 0.004  time: 11.772\n",
      "[EPOCH 9] loss: 0.002  time: 12.385\n",
      "[EPOCH 10] loss: 0.002  time: 11.351\n",
      "[EPOCH 11] loss: 0.003  time: 11.371\n",
      "[EPOCH 12] loss: 0.001  time: 11.101\n",
      "[EPOCH 13] loss: 0.002  time: 11.019\n",
      "[EPOCH 14] loss: 0.002  time: 11.644\n",
      "[EPOCH 15] loss: 0.001  time: 11.821\n",
      "[EPOCH 16] loss: 0.001  time: 13.966\n",
      "[EPOCH 17] loss: 0.002  time: 11.137\n",
      "[EPOCH 18] loss: 0.001  time: 13.813\n",
      "[EPOCH 19] loss: 0.001  time: 15.790\n",
      "[EPOCH 20] loss: 0.002  time: 11.974\n",
      "[EPOCH 21] loss: 0.001  time: 11.414\n",
      "[EPOCH 22] loss: 0.001  time: 21.190\n",
      "[EPOCH 23] loss: 0.001  time: 14.489\n",
      "[EPOCH 24] loss: 0.001  time: 11.845\n",
      "[EPOCH 25] loss: 0.001  time: 16.501\n",
      "[EPOCH 26] loss: 0.001  time: 15.074\n",
      "[EPOCH 27] loss: 0.001  time: 24.919\n",
      "[EPOCH 28] loss: 0.001  time: 15.687\n",
      "[EPOCH 29] loss: 0.001  time: 13.951\n",
      "[EPOCH 30] loss: 0.001  time: 13.416\n"
     ]
    }
   ],
   "source": [
    "train(net, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "[EPOCH 1] loss: 251.382  time: 2.180\n",
      "[EPOCH 2] loss: 251.355  time: 2.901\n",
      "[EPOCH 3] loss: 251.306  time: 1.802\n",
      "[EPOCH 4] loss: 251.236  time: 1.707\n",
      "[EPOCH 5] loss: 251.149  time: 1.702\n",
      "[EPOCH 6] loss: 251.050  time: 1.689\n",
      "[EPOCH 7] loss: 250.942  time: 1.688\n",
      "[EPOCH 8] loss: 250.827  time: 2.249\n",
      "[EPOCH 9] loss: 250.707  time: 2.291\n",
      "[EPOCH 10] loss: 250.586  time: 1.739\n",
      "[EPOCH 11] loss: 250.469  time: 1.735\n",
      "[EPOCH 12] loss: 250.353  time: 1.723\n",
      "[EPOCH 13] loss: 250.227  time: 1.728\n",
      "[EPOCH 14] loss: 250.090  time: 2.126\n",
      "[EPOCH 15] loss: 249.941  time: 2.104\n",
      "[EPOCH 16] loss: 249.780  time: 1.878\n",
      "[EPOCH 17] loss: 249.606  time: 1.937\n",
      "[EPOCH 18] loss: 249.416  time: 1.971\n",
      "[EPOCH 19] loss: 249.210  time: 1.973\n",
      "[EPOCH 20] loss: 248.987  time: 2.150\n",
      "[EPOCH 21] loss: 248.744  time: 1.819\n",
      "[EPOCH 22] loss: 248.480  time: 2.010\n",
      "[EPOCH 23] loss: 248.194  time: 2.034\n",
      "[EPOCH 24] loss: 247.882  time: 1.997\n",
      "[EPOCH 25] loss: 247.544  time: 1.831\n",
      "[EPOCH 26] loss: 247.176  time: 1.983\n",
      "[EPOCH 27] loss: 246.776  time: 1.990\n",
      "[EPOCH 28] loss: 246.340  time: 2.059\n",
      "[EPOCH 29] loss: 245.865  time: 2.012\n",
      "[EPOCH 30] loss: 245.348  time: 2.219\n",
      "[EPOCH 31] loss: 244.783  time: 1.901\n",
      "[EPOCH 32] loss: 244.167  time: 2.101\n",
      "[EPOCH 33] loss: 243.493  time: 2.147\n",
      "[EPOCH 34] loss: 242.757  time: 2.484\n",
      "[EPOCH 35] loss: 241.952  time: 2.513\n",
      "[EPOCH 36] loss: 241.070  time: 1.806\n",
      "[EPOCH 37] loss: 240.105  time: 2.093\n",
      "[EPOCH 38] loss: 239.047  time: 2.239\n",
      "[EPOCH 39] loss: 237.888  time: 2.100\n",
      "[EPOCH 40] loss: 236.615  time: 1.732\n",
      "[EPOCH 41] loss: 235.219  time: 2.017\n",
      "[EPOCH 42] loss: 233.686  time: 2.110\n",
      "[EPOCH 43] loss: 232.003  time: 1.796\n",
      "[EPOCH 44] loss: 230.155  time: 2.470\n",
      "[EPOCH 45] loss: 228.128  time: 2.332\n",
      "[EPOCH 46] loss: 225.905  time: 1.777\n",
      "[EPOCH 47] loss: 223.469  time: 1.750\n",
      "[EPOCH 48] loss: 220.805  time: 2.070\n",
      "[EPOCH 49] loss: 217.897  time: 2.086\n",
      "[EPOCH 50] loss: 214.732  time: 2.311\n",
      "[EPOCH 51] loss: 211.299  time: 2.002\n",
      "[EPOCH 52] loss: 207.592  time: 1.705\n",
      "[EPOCH 53] loss: 203.614  time: 2.498\n",
      "[EPOCH 54] loss: 199.375  time: 2.056\n",
      "[EPOCH 55] loss: 194.901  time: 1.785\n",
      "[EPOCH 56] loss: 190.231  time: 2.357\n",
      "[EPOCH 57] loss: 185.427  time: 1.749\n",
      "[EPOCH 58] loss: 180.573  time: 1.708\n",
      "[EPOCH 59] loss: 175.776  time: 1.730\n",
      "[EPOCH 60] loss: 171.169  time: 1.716\n",
      "[EPOCH 61] loss: 166.903  time: 1.887\n",
      "[EPOCH 62] loss: 163.137  time: 1.730\n",
      "[EPOCH 63] loss: 160.023  time: 1.715\n",
      "[EPOCH 64] loss: 157.685  time: 1.713\n",
      "[EPOCH 65] loss: 156.193  time: 1.714\n",
      "[EPOCH 66] loss: 155.542  time: 1.721\n",
      "[EPOCH 67] loss: 155.634  time: 1.719\n",
      "[EPOCH 68] loss: 156.284  time: 1.880\n",
      "[EPOCH 69] loss: 157.242  time: 1.905\n",
      "[EPOCH 70] loss: 158.227  time: 1.863\n",
      "[EPOCH 71] loss: 158.984  time: 1.998\n",
      "[EPOCH 72] loss: 159.329  time: 1.682\n",
      "[EPOCH 73] loss: 159.173  time: 1.670\n",
      "[EPOCH 74] loss: 158.533  time: 1.676\n",
      "[EPOCH 75] loss: 157.508  time: 1.864\n",
      "[EPOCH 76] loss: 156.246  time: 1.686\n",
      "[EPOCH 77] loss: 154.902  time: 1.667\n",
      "[EPOCH 78] loss: 153.611  time: 1.671\n",
      "[EPOCH 79] loss: 152.469  time: 1.679\n",
      "[EPOCH 80] loss: 151.527  time: 1.679\n",
      "[EPOCH 81] loss: 150.795  time: 2.261\n",
      "[EPOCH 82] loss: 150.254  time: 1.966\n",
      "[EPOCH 83] loss: 149.867  time: 1.880\n",
      "[EPOCH 84] loss: 149.586  time: 3.008\n",
      "[EPOCH 85] loss: 149.365  time: 3.640\n",
      "[EPOCH 86] loss: 149.163  time: 2.054\n",
      "[EPOCH 87] loss: 148.946  time: 1.983\n",
      "[EPOCH 88] loss: 148.690  time: 1.731\n",
      "[EPOCH 89] loss: 148.380  time: 2.054\n",
      "[EPOCH 90] loss: 148.010  time: 2.610\n",
      "[EPOCH 91] loss: 147.581  time: 2.056\n",
      "[EPOCH 92] loss: 147.099  time: 1.731\n",
      "[EPOCH 93] loss: 146.574  time: 1.746\n",
      "[EPOCH 94] loss: 146.018  time: 1.874\n",
      "[EPOCH 95] loss: 145.445  time: 1.873\n",
      "[EPOCH 96] loss: 144.867  time: 1.853\n",
      "[EPOCH 97] loss: 144.294  time: 1.766\n",
      "[EPOCH 98] loss: 143.736  time: 1.756\n",
      "[EPOCH 99] loss: 143.196  time: 1.933\n",
      "[EPOCH 100] loss: 142.676  time: 1.755\n",
      "[EPOCH 101] loss: 142.173  time: 1.824\n",
      "[EPOCH 102] loss: 141.684  time: 1.993\n",
      "[EPOCH 103] loss: 141.201  time: 1.845\n",
      "[EPOCH 104] loss: 140.719  time: 1.772\n",
      "[EPOCH 105] loss: 140.228  time: 1.733\n",
      "[EPOCH 106] loss: 139.724  time: 1.814\n",
      "[EPOCH 107] loss: 139.203  time: 2.145\n",
      "[EPOCH 108] loss: 138.661  time: 2.260\n",
      "[EPOCH 109] loss: 138.097  time: 1.786\n",
      "[EPOCH 110] loss: 137.513  time: 1.745\n",
      "[EPOCH 111] loss: 136.911  time: 1.704\n",
      "[EPOCH 112] loss: 136.293  time: 1.681\n",
      "[EPOCH 113] loss: 135.661  time: 1.964\n",
      "[EPOCH 114] loss: 135.020  time: 1.700\n",
      "[EPOCH 115] loss: 134.369  time: 2.102\n",
      "[EPOCH 116] loss: 133.710  time: 2.053\n",
      "[EPOCH 117] loss: 133.043  time: 2.614\n",
      "[EPOCH 118] loss: 132.366  time: 1.800\n",
      "[EPOCH 119] loss: 131.680  time: 1.807\n",
      "[EPOCH 120] loss: 130.980  time: 1.843\n",
      "[EPOCH 121] loss: 130.267  time: 1.729\n",
      "[EPOCH 122] loss: 129.537  time: 1.766\n",
      "[EPOCH 123] loss: 128.790  time: 1.728\n",
      "[EPOCH 124] loss: 128.023  time: 1.733\n",
      "[EPOCH 125] loss: 127.236  time: 1.920\n",
      "[EPOCH 126] loss: 126.428  time: 1.705\n",
      "[EPOCH 127] loss: 125.598  time: 1.705\n",
      "[EPOCH 128] loss: 124.747  time: 1.709\n",
      "[EPOCH 129] loss: 123.874  time: 1.715\n",
      "[EPOCH 130] loss: 122.979  time: 1.700\n",
      "[EPOCH 131] loss: 122.061  time: 1.713\n",
      "[EPOCH 132] loss: 121.122  time: 1.731\n",
      "[EPOCH 133] loss: 120.160  time: 1.727\n",
      "[EPOCH 134] loss: 119.175  time: 1.921\n",
      "[EPOCH 135] loss: 118.166  time: 1.715\n",
      "[EPOCH 136] loss: 117.134  time: 1.707\n",
      "[EPOCH 137] loss: 116.077  time: 1.700\n",
      "[EPOCH 138] loss: 114.994  time: 1.759\n",
      "[EPOCH 139] loss: 113.886  time: 1.727\n",
      "[EPOCH 140] loss: 112.753  time: 1.708\n",
      "[EPOCH 141] loss: 111.593  time: 1.971\n",
      "[EPOCH 142] loss: 110.409  time: 1.787\n",
      "[EPOCH 143] loss: 109.200  time: 1.698\n",
      "[EPOCH 144] loss: 107.966  time: 1.710\n",
      "[EPOCH 145] loss: 106.708  time: 1.717\n",
      "[EPOCH 146] loss: 105.427  time: 1.917\n",
      "[EPOCH 147] loss: 104.124  time: 1.740\n",
      "[EPOCH 148] loss: 102.801  time: 1.902\n",
      "[EPOCH 149] loss: 101.459  time: 1.763\n",
      "[EPOCH 150] loss: 100.097  time: 1.708\n",
      "[EPOCH 151] loss: 98.717  time: 1.738\n",
      "[EPOCH 152] loss: 97.320  time: 1.905\n",
      "[EPOCH 153] loss: 95.904  time: 1.734\n",
      "[EPOCH 154] loss: 94.470  time: 1.712\n",
      "[EPOCH 155] loss: 93.022  time: 1.705\n",
      "[EPOCH 156] loss: 91.560  time: 1.705\n",
      "[EPOCH 157] loss: 90.089  time: 1.714\n",
      "[EPOCH 158] loss: 88.615  time: 1.701\n",
      "[EPOCH 159] loss: 87.141  time: 1.708\n",
      "[EPOCH 160] loss: 85.672  time: 1.713\n",
      "[EPOCH 161] loss: 84.213  time: 1.892\n",
      "[EPOCH 162] loss: 82.766  time: 1.905\n",
      "[EPOCH 163] loss: 81.333  time: 1.785\n",
      "[EPOCH 164] loss: 79.916  time: 2.017\n",
      "[EPOCH 165] loss: 78.519  time: 1.857\n",
      "[EPOCH 166] loss: 77.146  time: 1.887\n",
      "[EPOCH 167] loss: 75.801  time: 1.808\n",
      "[EPOCH 168] loss: 74.487  time: 1.812\n",
      "[EPOCH 169] loss: 73.210  time: 1.812\n",
      "[EPOCH 170] loss: 71.972  time: 2.003\n",
      "[EPOCH 171] loss: 70.780  time: 1.842\n",
      "[EPOCH 172] loss: 69.635  time: 1.971\n",
      "[EPOCH 173] loss: 68.542  time: 1.829\n",
      "[EPOCH 174] loss: 67.506  time: 1.809\n",
      "[EPOCH 175] loss: 66.527  time: 1.870\n",
      "[EPOCH 176] loss: 65.609  time: 1.803\n",
      "[EPOCH 177] loss: 64.751  time: 1.796\n",
      "[EPOCH 178] loss: 63.956  time: 1.839\n",
      "[EPOCH 179] loss: 63.225  time: 1.858\n",
      "[EPOCH 180] loss: 62.556  time: 1.972\n",
      "[EPOCH 181] loss: 61.948  time: 1.980\n",
      "[EPOCH 182] loss: 61.399  time: 1.797\n",
      "[EPOCH 183] loss: 60.909  time: 1.866\n",
      "[EPOCH 184] loss: 60.473  time: 1.933\n",
      "[EPOCH 185] loss: 60.086  time: 1.793\n",
      "[EPOCH 186] loss: 59.746  time: 2.011\n",
      "[EPOCH 187] loss: 59.451  time: 2.362\n",
      "[EPOCH 188] loss: 59.195  time: 1.882\n",
      "[EPOCH 189] loss: 58.977  time: 2.189\n",
      "[EPOCH 190] loss: 58.792  time: 1.844\n",
      "[EPOCH 191] loss: 58.638  time: 1.884\n",
      "[EPOCH 192] loss: 58.510  time: 2.134\n",
      "[EPOCH 193] loss: 58.406  time: 1.747\n",
      "[EPOCH 194] loss: 58.322  time: 2.317\n",
      "[EPOCH 195] loss: 58.256  time: 1.785\n",
      "[EPOCH 196] loss: 58.204  time: 1.951\n",
      "[EPOCH 197] loss: 58.164  time: 1.902\n",
      "[EPOCH 198] loss: 58.133  time: 1.726\n",
      "[EPOCH 199] loss: 58.109  time: 2.071\n",
      "[EPOCH 200] loss: 58.091  time: 2.244\n",
      "=========================================\n",
      "Training Completed...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1d1b4d63f470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-9de494a2d5b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=========================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Completed...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[FINAL] loss: %.3f  time: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "train(net, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_error(net, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: -1.5149315567016601\n",
      "Abs Error: 52.31066065979004\n"
     ]
    }
   ],
   "source": [
    "accum = 0\n",
    "accum1 = 0\n",
    "for i in range(1000):\n",
    "    error = net(train_ds[i][\"sample\"]).item() - train_ds[i][\"label\"].item()\n",
    "    accum += error\n",
    "    accum1 += abs(error)\n",
    "print(\"Error:\", accum/1000)\n",
    "print(\"Abs Error:\", accum1/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(len(data[\"label\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[4][\"label\"].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
