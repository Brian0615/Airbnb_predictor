{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Req'd Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Getting File Names & Saving csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all the file_names in a given directory\n",
    "def get_file_names(folder):\n",
    "    #Listing entries present in given folder\n",
    "    entries = os.listdir(folder)\n",
    "    for i in entries:\n",
    "        if 'csv' not in i:\n",
    "            entries.remove(i)\n",
    "    return sorted(entries, reverse=True)\n",
    "\n",
    "#Saving the file\n",
    "def save_file(root, name_of_file, my_dataframe):\n",
    "    #Test if save directory exists\n",
    "    try:\n",
    "        my_dataframe.to_csv(root+'processed_data/'+ name_of_file, index=False)\n",
    "    #Otherwise make the directory and then save\n",
    "    except:\n",
    "        os.mkdir(root+'processed_data')\n",
    "        my_dataframe.to_csv(root+'processed_data/'+ name_of_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Initialization of File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(filepath):\n",
    "    df = pd.read_csv(filepath, \n",
    "                     usecols = ['id','last_scraped', 'host_is_superhost', \n",
    "                            'neighbourhood_cleansed', 'property_type',\n",
    "                           'room_type','accommodates','bathrooms',\n",
    "                           'bedrooms','beds','amenities', 'price',\n",
    "                           'minimum_nights','maximum_nights',\n",
    "                            'instant_bookable','cancellation_policy'])\n",
    "    df[\"last_scraped\"] = pd.to_datetime(df[\"last_scraped\"])\n",
    "    df[\"price\"] = df[\"price\"].apply(lambda x: x.replace('$','').replace(',', '').replace('.00', '')).astype(\"int\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Removing Missing Data and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove missing data\n",
    "def missing_data(my_dataframe):\n",
    "    #computing number of total rows\n",
    "    total_rows = my_dataframe.shape[0]\n",
    "    #Dropping rows if empty cells are present in any of the given rows\n",
    "    my_dataframe = my_dataframe.dropna(subset = ['id','last_scraped', 'host_is_superhost', \n",
    "                            'neighbourhood_cleansed', 'property_type',\n",
    "                           'room_type','accommodates','bathrooms',\n",
    "                           'bedrooms','beds','amenities', 'price',\n",
    "                           'minimum_nights','maximum_nights',\n",
    "                            'instant_bookable','cancellation_policy'])\n",
    "    #Computing number of rows left after removing rows for error checking\n",
    "    total_rows_after = my_dataframe.shape[0]\n",
    "    \n",
    "    #Printing\n",
    "    print('total rows before missing data removal:', total_rows)\n",
    "    print('total rows after missing data removal:', total_rows_after)\n",
    "    \n",
    "    return my_dataframe\n",
    "\n",
    "\n",
    "def remove_outlier(my_dataframe):\n",
    "    \n",
    "    #Plotting a box and whisker plot for data visualization\n",
    "    #sns.boxplot(x=my_dataframe['price'])\n",
    "    #Computing the quantiles of the box and whisker plot\n",
    "    quantiles = my_dataframe['price'].quantile([0.0001, 0.01,0.05, 0.25,0.5,0.75,0.90, 0.95,0.99])\n",
    "    \n",
    "    #computing number of total rows prior to removing outliers\n",
    "    total_rows = my_dataframe.shape[0]\n",
    "    print('Number of rows before outlier removal',total_rows)\n",
    "    \n",
    "    #Adjusting data based on quantile information\n",
    "    my_new_dataframe = my_dataframe[(my_dataframe['price'] < quantiles.loc[0.95]) & (my_dataframe['price'] > quantiles.loc[0.05])]\n",
    "    quantiles = my_new_dataframe['price'].quantile([0.0001, 0.01,0.05, 0.25,0.5,0.75,0.90, 0.95,0.99])\n",
    "    sns.boxplot(x=my_new_dataframe['price'])\n",
    "    \n",
    "    #computing new number of total rows\n",
    "    total_rows = my_new_dataframe.shape[0]\n",
    "    \n",
    "    print('Number of rows after outlier removal',total_rows)\n",
    "\n",
    "    return my_new_dataframe\n",
    "\n",
    "#Function for normalizing data \n",
    "def nomarlize_price(my_dataframe):\n",
    "    my_dataframe['price'] = (my_dataframe['price'] - \n",
    "                             my_dataframe['price'].min())/(my_dataframe['price'].max() - \n",
    "                                                         my_dataframe['price'].min())\n",
    "    return my_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Mapping Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map neighbourhood names to modified MLS district codes\n",
    "def map_neighbourhoods(data, n_data):\n",
    "    \n",
    "    district = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district\"].tolist()))\n",
    "    district_group = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district_group\"].tolist()))\n",
    "    district_number = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district_number\"].tolist()))\n",
    "\n",
    "    \n",
    "    data[\"district_group\"] = data[\"neighbourhood_cleansed\"].map(district_group)\n",
    "    data[\"district_number\"] = data[\"neighbourhood_cleansed\"].map(district_number)\n",
    "    data[\"district\"] = data[\"district_group\"]*15+data[\"district_number\"]\n",
    "    return data\n",
    "\n",
    "# Convert categorical data (e.g. host_is_superhost) into numerical codes\n",
    "def cat_to_code(data, root, categories):\n",
    "    category_codes = {}\n",
    "    \n",
    "    for i in categories:\n",
    "        name = data[i].name\n",
    "        category_codes[name] = data[i].astype(\"category\").cat.categories # save list of categories\n",
    "        data[name+\"_codes\"] = data[i].astype(\"category\").cat.codes # convert to code\n",
    "    \n",
    "    for i in category_codes:\n",
    "        df = pd.DataFrame()\n",
    "        df[i] = category_codes[i]\n",
    "        df.to_csv(root+r\"category_codes/\"+i+\".csv\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Filter out categories with a small percentage (<5% of data)\n",
    "def filter_categorical(data, properties, can_policy):\n",
    "    num_samples = data.shape[0]\n",
    "    data = data[data.property_type.isin(properties)]\n",
    "    num_samples2 = data.shape[0]\n",
    "    data = data[data.cancellation_policy.isin(can_policy)]\n",
    "    num_samples3 = data.shape[0]\n",
    "    removed = {\"properties\": num_samples-num_samples2,\n",
    "               \"can_policy\": num_samples2-num_samples3}\n",
    "    print(\"Number removed:\")\n",
    "    print(removed)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Filtering of Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amenityRemoval(df, toKeep):\n",
    "    toKeep = ['TV', 'Internet', 'Wifi', 'Air conditioning', 'Kitchen', 'Indoor fireplace', 'Heating', \n",
    "          'Family/kid friendly', 'Washer', 'Dryer', 'Smoke detector', 'First aid kit', 'Fire extinguisher', \n",
    "          'Essentials', 'Shampoo', 'Cable TV', 'Pool', 'Free parking on premises', 'Doorman', 'Gym', 'Elevator', \n",
    "          'Buzzer/wireless intercom', 'Free street parking', 'Carbon monoxide detector', 'Lock on bedroom door', \n",
    "          '24-hour check-in', 'Hangers', 'Hair dryer', 'Iron', 'Laptop friendly workspace',  \n",
    "          'Self check-in', 'Keypad', 'Private entrance', 'Hot water', 'Bed linens', 'Extra pillows and blankets', \n",
    "          'Microwave', 'Coffee maker', 'Refrigerator', 'Dishes and silverware', 'Cooking basics', 'Oven', 'Stove', \n",
    "          'Long term stays allowed', 'No stairs or steps to enter', 'Wheelchair accessible', 'Pets allowed', \n",
    "          'Dishwasher', 'Single level home', 'Patio or balcony', 'Luggage dropoff allowed', \n",
    "          'Paid parking on premises', 'Host greets you', 'Lockbox', 'Garden or backyard', \n",
    "          'Paid parking off premises', 'Private living room', 'Breakfast', 'Hot tub', \n",
    "          'Wide hallways', 'Safety card', 'Bathtub', 'BBQ grill', 'Room-darkening shades', \n",
    "          'Well-lit path to entrance', 'Lake access', 'Ethernet connection', 'Wide entrance for guests', \n",
    "          'Flat path to guest entrance', 'Extra space around bed', 'Wide entrance']\n",
    "    amen = []\n",
    "    \n",
    "    for p in range(len(df)):\n",
    "        amen = df[\"amenities\"][p]\n",
    "        amen = amen[1:len(amen)-1]\n",
    "        amen = str(amen)\n",
    "        amen = amen.replace(\"\\\"\", \"\")\n",
    "        amen = amen.split(\",\")\n",
    "\n",
    "    for i in amen:\n",
    "        if i not in toKeep:\n",
    "            amen.remove(i)   \n",
    "\n",
    "    for j in toKeep:\n",
    "        df[\"amenities_\"+j] = df[\"amenities\"].apply(lambda x: j in x).astype(int)\n",
    "    \n",
    "    df = df.drop(columns=\"amenities\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  19_08_08_listings.csv\n",
      "--Start # of Samples: 21617\n",
      "total rows before missing data removal: 21617\n",
      "total rows after missing data removal: 21580\n",
      "Number of rows before outlier removal 21580\n",
      "Number of rows after outlier removal 19247\n",
      "--End # of Samples: 19247\n",
      "--Total # of Samples Removed 2370\n",
      "--Accumulated # of Samples: 19247\n",
      "Processing:  19_07_08_listings.csv\n",
      "--Start # of Samples: 21312\n",
      "total rows before missing data removal: 21312\n",
      "total rows after missing data removal: 21265\n",
      "Number of rows before outlier removal 21265\n",
      "Number of rows after outlier removal 18954\n",
      "--End # of Samples: 18954\n",
      "--Total # of Samples Removed 2358\n",
      "--Accumulated # of Samples: 38201\n",
      "Processing:  19_06_04_listings.csv\n",
      "--Start # of Samples: 20769\n",
      "total rows before missing data removal: 20769\n",
      "total rows after missing data removal: 20724\n",
      "Number of rows before outlier removal 20724\n",
      "Number of rows after outlier removal 18429\n",
      "--End # of Samples: 18429\n",
      "--Total # of Samples Removed 2340\n",
      "--Accumulated # of Samples: 56630\n",
      "Processing:  19_05_06_listings.csv\n",
      "--Start # of Samples: 20303\n",
      "total rows before missing data removal: 20303\n",
      "total rows after missing data removal: 20244\n",
      "Number of rows before outlier removal 20244\n",
      "Number of rows after outlier removal 18048\n",
      "--End # of Samples: 18048\n",
      "--Total # of Samples Removed 2255\n",
      "--Accumulated # of Samples: 74678\n",
      "Processing:  19_04_08_listings.csv\n",
      "--Start # of Samples: 20143\n",
      "total rows before missing data removal: 20143\n",
      "total rows after missing data removal: 20094\n",
      "Number of rows before outlier removal 20094\n",
      "Number of rows after outlier removal 17845\n",
      "--End # of Samples: 17845\n",
      "--Total # of Samples Removed 2298\n",
      "--Accumulated # of Samples: 92523\n",
      "Processing:  19_03_07_listings.csv\n",
      "--Start # of Samples: 20036\n",
      "total rows before missing data removal: 20036\n",
      "total rows after missing data removal: 19990\n",
      "Number of rows before outlier removal 19990\n",
      "Number of rows after outlier removal 17684\n",
      "--End # of Samples: 17684\n",
      "--Total # of Samples Removed 2352\n",
      "--Accumulated # of Samples: 110207\n",
      "Processing:  19_02_04_listings.csv\n",
      "--Start # of Samples: 19885\n",
      "total rows before missing data removal: 19885\n",
      "total rows after missing data removal: 19846\n",
      "Number of rows before outlier removal 19846\n",
      "Number of rows after outlier removal 17526\n",
      "--End # of Samples: 17526\n",
      "--Total # of Samples Removed 2359\n",
      "--Accumulated # of Samples: 127733\n",
      "Processing:  19_01_13_listings.csv\n",
      "--Start # of Samples: 19653\n",
      "total rows before missing data removal: 19653\n",
      "total rows after missing data removal: 19615\n",
      "Number of rows before outlier removal 19615\n",
      "Number of rows after outlier removal 17417\n",
      "--End # of Samples: 17417\n",
      "--Total # of Samples Removed 2236\n",
      "--Accumulated # of Samples: 145150\n",
      "Processing:  18_12_06_listings.csv\n",
      "--Start # of Samples: 19255\n",
      "total rows before missing data removal: 19255\n",
      "total rows after missing data removal: 19219\n",
      "Number of rows before outlier removal 19219\n",
      "Number of rows after outlier removal 17052\n",
      "--End # of Samples: 17052\n",
      "--Total # of Samples Removed 2203\n",
      "--Accumulated # of Samples: 162202\n",
      "Processing:  18_11_04_listings.csv\n",
      "--Start # of Samples: 18509\n",
      "total rows before missing data removal: 18509\n",
      "total rows after missing data removal: 18470\n",
      "Number of rows before outlier removal 18470\n",
      "Number of rows after outlier removal 16341\n",
      "--End # of Samples: 16341\n",
      "--Total # of Samples Removed 2168\n",
      "--Accumulated # of Samples: 178543\n",
      "Processing:  18_10_06_listings.csv\n",
      "--Start # of Samples: 17343\n",
      "total rows before missing data removal: 17343\n",
      "total rows after missing data removal: 17258\n",
      "Number of rows before outlier removal 17258\n",
      "Number of rows after outlier removal 15361\n",
      "--End # of Samples: 15361\n",
      "--Total # of Samples Removed 1982\n",
      "--Accumulated # of Samples: 193904\n",
      "Processing:  18_09_08_listings.csv\n",
      "--Start # of Samples: 17682\n",
      "total rows before missing data removal: 17682\n",
      "total rows after missing data removal: 17643\n",
      "Number of rows before outlier removal 17643\n",
      "Number of rows after outlier removal 15636\n",
      "--End # of Samples: 15636\n",
      "--Total # of Samples Removed 2046\n",
      "--Accumulated # of Samples: 209540\n",
      "===========================================\n",
      "Data Processing Complete\n",
      "--Total # of Samples Processed:  236507\n",
      "--Total # of Samples Kept:  209540 (88.59780048793482%)\n",
      "--Total # of Samples Removed:  26967 (11.402199512065183%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get list of filenames\n",
    "root = '../'\n",
    "\n",
    "# Categories to convert to code\n",
    "categories = [\"host_is_superhost\", \"property_type\", \"room_type\", \"instant_bookable\", \"cancellation_policy\"]\n",
    "\n",
    "#Properties and cancellation policies to filter\n",
    "properties = [\"Apartment\", \"Condominium\", \"House\", \"Townhouse\", \"Guest suite\", \"Bungalow\"]\n",
    "can_policy = [\"flexible\", \"moderate\", \"strict_14_with_grace_period\"]\n",
    "\n",
    "#mapping for neighbourhoods\n",
    "n_data = pd.read_csv(root+'category_codes/neighbourhoods.csv')\n",
    "\n",
    "#List of Amenities to Keep -> has more than 5%\n",
    "amenToKeep = ['TV', 'Internet', 'Wifi', 'Air conditioning', 'Kitchen', 'Indoor fireplace', 'Heating', \n",
    "          'Family/kid friendly', 'Washer', 'Dryer', 'Smoke detector', 'First aid kit', 'Fire extinguisher', \n",
    "          'Essentials', 'Shampoo', 'Cable TV', 'Pool', 'Free parking on premises', 'Doorman', 'Gym', 'Elevator', \n",
    "          'Buzzer/wireless intercom', 'Free street parking', 'Carbon monoxide detector', 'Lock on bedroom door', \n",
    "          '24-hour check-in', 'Hangers', 'Hair dryer', 'Iron', 'Laptop friendly workspace',  \n",
    "          'Self check-in', 'Keypad', 'Private entrance', 'Hot water', 'Bed linens', 'Extra pillows and blankets', \n",
    "          'Microwave', 'Coffee maker', 'Refrigerator', 'Dishes and silverware', 'Cooking basics', 'Oven', 'Stove', \n",
    "          'Long term stays allowed', 'No stairs or steps to enter', 'Wheelchair accessible', 'Pets allowed', \n",
    "          'Dishwasher', 'Single level home', 'Patio or balcony', 'Luggage dropoff allowed', \n",
    "          'Paid parking on premises', 'Host greets you', 'Lockbox', 'Garden or backyard', \n",
    "          'Paid parking off premises', 'Private living room', 'Breakfast', 'Hot tub', \n",
    "          'Wide hallways', 'Safety card', 'Bathtub', 'BBQ grill', 'Room-darkening shades', \n",
    "          'Well-lit path to entrance', 'Lake access', 'Ethernet connection', 'Wide entrance for guests', \n",
    "          'Flat path to guest entrance', 'Extra space around bed', 'Wide entrance']\n",
    "\n",
    "#obtain list of files\n",
    "name = get_file_names(root+'original_dataset')\n",
    "accum_kept = 0\n",
    "accum_removed = 0\n",
    "for i in name:\n",
    "    print(\"Processing: \", i)\n",
    "    filepath = root+'original_dataset/'+i\n",
    "    df = initialization(filepath)\n",
    "    start_len = len(df)\n",
    "    print(\"--Start # of Samples:\", start_len)\n",
    "    \n",
    "    df = missing_data(df)\n",
    "    df = remove_outlier(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = amenityRemoval(df, amenToKeep)\n",
    "    #df = df.drop(\"amenities\")\n",
    "    df = pd.get_dummies(df)\n",
    "    #df = nomarlize_price(df)\n",
    "    #df = map_neighbourhoods(df, n_data)\n",
    "    #df = cat_to_code(df, root, categories)\n",
    "    #df = filter_categorical(df, properties, can_policy)\n",
    "    end_len = len(df)\n",
    "    print(\"--End # of Samples:\", end_len)\n",
    "    print(\"--Total # of Samples Removed\", start_len-end_len)\n",
    "    save_file(root, i, df)\n",
    "    accum_kept += end_len\n",
    "    accum_removed += start_len-end_len\n",
    "    print(\"--Accumulated # of Samples:\", accum_kept)\n",
    "\n",
    "print(\"===========================================\")\n",
    "print(\"Data Processing Complete\")\n",
    "print(\"--Total # of Samples Processed: \", accum_kept+accum_removed)\n",
    "print(\"--Total # of Samples Kept: \", accum_kept, \"(\"+str(accum_kept/(accum_kept+accum_removed)*100)+\"%)\")\n",
    "print(\"--Total # of Samples Removed: \", accum_removed, \"(\"+str(accum_removed/(accum_kept+accum_removed)*100)+\"%)\")\n",
    "    \n",
    "#save_file(name[1],df)\n",
    "#df = missing_data(df)\n",
    "#df = remove_outlier(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../processed_data/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root+\"processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17248, 263)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
