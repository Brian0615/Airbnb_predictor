{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Req'd Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Getting File Names & Saving csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all the file_names in a given directory\n",
    "def get_file_names(folder):\n",
    "    #Listing entries present in given folder\n",
    "    entries = os.listdir(folder)\n",
    "    for i in entries:\n",
    "        if 'csv' not in i:\n",
    "            entries.remove(i)\n",
    "    return sorted(entries, reverse=True)\n",
    "\n",
    "#Saving the file\n",
    "def save_file(root, name_of_file,my_dataframe):\n",
    "    #Test if save directory exists\n",
    "    try:\n",
    "        my_dataframe.to_csv(root+'processed_data/'+ name_of_file)\n",
    "    #Otherwise make the directory and then save\n",
    "    except:\n",
    "        os.mkdir(root+'processed_data')\n",
    "        my_dataframe.to_csv(root+'processed_data/'+ name_of_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Initialization of File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(filepath):\n",
    "    df = pd.read_csv(filepath, \n",
    "                     usecols = ['id','last_scraped', 'host_is_superhost', \n",
    "                            'neighbourhood_cleansed', 'property_type',\n",
    "                           'room_type','accommodates','bathrooms',\n",
    "                           'bedrooms','beds','amenities', 'price',\n",
    "                           'minimum_nights','maximum_nights',\n",
    "                            'instant_bookable','cancellation_policy'])\n",
    "    df[\"last_scraped\"] = pd.to_datetime(df[\"last_scraped\"])\n",
    "    df[\"price\"] = df[\"price\"].apply(lambda x: x.replace('$','').replace(',', '').replace('.00', '')).astype(\"int\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Removing Missing Data and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove missing data\n",
    "def missing_data(my_dataframe):\n",
    "    #computing number of total rows\n",
    "    total_rows = my_dataframe.shape[0]\n",
    "    #Dropping rows if empty cells are present in any of the given rows\n",
    "    my_dataframe = my_dataframe.dropna(subset = ['id','last_scraped', 'host_is_superhost', \n",
    "                            'neighbourhood_cleansed', 'property_type',\n",
    "                           'room_type','accommodates','bathrooms',\n",
    "                           'bedrooms','beds','amenities', 'price',\n",
    "                           'minimum_nights','maximum_nights',\n",
    "                            'instant_bookable','cancellation_policy'])\n",
    "    #Computing number of rows left after removing rows for error checking\n",
    "    total_rows_after = my_dataframe.shape[0]\n",
    "    \n",
    "    #Printing\n",
    "    print('total rows before missing data removal:', total_rows)\n",
    "    print('total rows after missing data removal:', total_rows_after)\n",
    "    \n",
    "    return my_dataframe\n",
    "\n",
    "\n",
    "def remove_outlier(my_dataframe):\n",
    "    \n",
    "    #Plotting a box and whisker plot for data visualization\n",
    "    sns.boxplot(x=my_dataframe['price'])\n",
    "    #Computing the quantiles of the box and whisker plot\n",
    "    quantiles = my_dataframe['price'].quantile([0.0001, 0.01,0.25,0.5,0.75,0.95,0.99])\n",
    "    \n",
    "    #computing number of total rows prior to removing outliers\n",
    "    total_rows = my_dataframe.shape[0]\n",
    "    print('Number of rows before outlier removal',total_rows)\n",
    "    \n",
    "    #Adjusting data based on quantile information\n",
    "    my_new_dataframe = my_dataframe[(my_dataframe['price'] < quantiles.loc[0.99]) & (my_dataframe['price'] > quantiles.loc[0.01])]\n",
    "    quantiles = my_new_dataframe['price'].quantile([0.0001, 0.01,0.25,0.5,0.75,0.95,0.99])\n",
    "    \n",
    "    #computing new number of total rows\n",
    "    total_rows = my_new_dataframe.shape[0]\n",
    "    \n",
    "    print('Number of rows after outlier removal',total_rows)\n",
    "\n",
    "    return my_new_dataframe\n",
    "\n",
    "#Function for normalizing data \n",
    "def nomarlize_price(my_dataframe):\n",
    "    my_dataframe['price'] = (my_dataframe['price'] - \n",
    "                             my_dataframe['price'].min())/(my_dataframe['price'].max() - \n",
    "                                                         my_dataframe['price'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Mapping Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map neighbourhood names to modified MLS district codes\n",
    "def map_neighbourhoods(data, n_data):\n",
    "    \n",
    "    district = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district\"].tolist()))\n",
    "    district_group = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district_group\"].tolist()))\n",
    "    district_number = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district_number\"].tolist()))\n",
    "\n",
    "    \n",
    "    data[\"district_group\"] = data[\"neighbourhood_cleansed\"].map(district_group)\n",
    "    data[\"district_number\"] = data[\"neighbourhood_cleansed\"].map(district_number)\n",
    "    data[\"district\"] = data[\"district_group\"]*15+data[\"district_number\"]\n",
    "    return data\n",
    "\n",
    "# Convert categorical data (e.g. host_is_superhost) into numerical codes\n",
    "def cat_to_code(data, root, categories):\n",
    "    category_codes = {}\n",
    "    \n",
    "    for i in categories:\n",
    "        name = data[i].name\n",
    "        category_codes[name] = data[i].astype(\"category\").cat.categories # save list of categories\n",
    "        data[name+\"_codes\"] = data[i].astype(\"category\").cat.codes # convert to code\n",
    "    \n",
    "    for i in category_codes:\n",
    "        df = pd.DataFrame()\n",
    "        df[i] = category_codes[i]\n",
    "        df.to_csv(root+r\"category_codes/\"+i+\".csv\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Filter out categories with a small percentage (<5% of data)\n",
    "def filter_categorical(data, properties, can_policy):\n",
    "    num_samples = data.shape[0]\n",
    "    data = data[data.property_type.isin(properties)]\n",
    "    num_samples2 = data.shape[0]\n",
    "    data = data[data.cancellation_policy.isin(can_policy)]\n",
    "    num_samples3 = data.shape[0]\n",
    "    removed = {\"properties\": num_samples-num_samples2,\n",
    "               \"can_policy\": num_samples2-num_samples3}\n",
    "    print(\"Number removed:\")\n",
    "    print(removed)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Filtering of Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amenityRemoval(df, toKeep):\n",
    "    toKeep = ['TV', 'Internet', 'Wifi', 'Air conditioning', 'Kitchen', 'Indoor fireplace', 'Heating', \n",
    "          'Family/kid friendly', 'Washer', 'Dryer', 'Smoke detector', 'First aid kit', 'Fire extinguisher', \n",
    "          'Essentials', 'Shampoo', 'Cable TV', 'Pool', 'Free parking on premises', 'Doorman', 'Gym', 'Elevator', \n",
    "          'Buzzer/wireless intercom', 'Free street parking', 'Carbon monoxide detector', 'Lock on bedroom door', \n",
    "          '24-hour check-in', 'Hangers', 'Hair dryer', 'Iron', 'Laptop friendly workspace',  \n",
    "          'Self check-in', 'Keypad', 'Private entrance', 'Hot water', 'Bed linens', 'Extra pillows and blankets', \n",
    "          'Microwave', 'Coffee maker', 'Refrigerator', 'Dishes and silverware', 'Cooking basics', 'Oven', 'Stove', \n",
    "          'Long term stays allowed', 'No stairs or steps to enter', 'Wheelchair accessible', 'Pets allowed', \n",
    "          'Dishwasher', 'Single level home', 'Patio or balcony', 'Luggage dropoff allowed', \n",
    "          'Paid parking on premises', 'Host greets you', 'Lockbox', 'Garden or backyard', \n",
    "          'Paid parking off premises', 'Private living room', 'Breakfast', 'Hot tub', \n",
    "          'Wide hallways', 'Safety card', 'Bathtub', 'BBQ grill', 'Room-darkening shades', \n",
    "          'Well-lit path to entrance', 'Lake access', 'Ethernet connection', 'Wide entrance for guests', \n",
    "          'Flat path to guest entrance', 'Extra space around bed', 'Wide entrance']\n",
    "    amen = []\n",
    "    \n",
    "    for p in range(len(df)):\n",
    "        amen = df[\"amenities\"][p]\n",
    "        amen = amen[1:len(amen)-1]\n",
    "        amen = str(amen)\n",
    "        amen = amen.replace(\"\\\"\", \"\")\n",
    "        amen = amen.split(\",\")\n",
    "\n",
    "    for i in amen:\n",
    "        if i not in toKeep:\n",
    "            amen.remove(i)   \n",
    "\n",
    "    for j in toKeep:\n",
    "        df[j] = df[\"amenities\"].apply(lambda x: j in x).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  19_08_08_listings.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../full_dataset/19_08_08_listings.csv' does not exist: b'../full_dataset/19_08_08_listings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d094cb0e5c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'full_dataset/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mstart_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--Start # of Samples:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ffb5eb35a02b>\u001b[0m in \u001b[0;36minitialization\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      6\u001b[0m                            \u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'beds'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'amenities'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                            \u001b[0;34m'minimum_nights'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'maximum_nights'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                             'instant_bookable','cancellation_policy'])\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"last_scraped\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"last_scraped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"price\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"price\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../full_dataset/19_08_08_listings.csv' does not exist: b'../full_dataset/19_08_08_listings.csv'"
     ]
    }
   ],
   "source": [
    "# get list of filenames\n",
    "root = '../'\n",
    "\n",
    "# Categories to convert to code\n",
    "categories = [\"host_is_superhost\", \"property_type\", \"room_type\", \"instant_bookable\", \"cancellation_policy\"]\n",
    "\n",
    "#Properties and cancellation policies to filter\n",
    "properties = [\"Apartment\", \"Condominium\", \"House\", \"Townhouse\", \"Guest suite\", \"Bungalow\"]\n",
    "can_policy = [\"flexible\", \"moderate\", \"strict_14_with_grace_period\"]\n",
    "\n",
    "#mapping for neighbourhoods\n",
    "n_data = pd.read_csv(root+'category_codes/neighbourhoods.csv')\n",
    "\n",
    "#List of Amenities to Keep -> has more than 5%\n",
    "amenToKeep = ['TV', 'Internet', 'Wifi', 'Air conditioning', 'Kitchen', 'Indoor fireplace', 'Heating', \n",
    "          'Family/kid friendly', 'Washer', 'Dryer', 'Smoke detector', 'First aid kit', 'Fire extinguisher', \n",
    "          'Essentials', 'Shampoo', 'Cable TV', 'Pool', 'Free parking on premises', 'Doorman', 'Gym', 'Elevator', \n",
    "          'Buzzer/wireless intercom', 'Free street parking', 'Carbon monoxide detector', 'Lock on bedroom door', \n",
    "          '24-hour check-in', 'Hangers', 'Hair dryer', 'Iron', 'Laptop friendly workspace',  \n",
    "          'Self check-in', 'Keypad', 'Private entrance', 'Hot water', 'Bed linens', 'Extra pillows and blankets', \n",
    "          'Microwave', 'Coffee maker', 'Refrigerator', 'Dishes and silverware', 'Cooking basics', 'Oven', 'Stove', \n",
    "          'Long term stays allowed', 'No stairs or steps to enter', 'Wheelchair accessible', 'Pets allowed', \n",
    "          'Dishwasher', 'Single level home', 'Patio or balcony', 'Luggage dropoff allowed', \n",
    "          'Paid parking on premises', 'Host greets you', 'Lockbox', 'Garden or backyard', \n",
    "          'Paid parking off premises', 'Private living room', 'Breakfast', 'Hot tub', \n",
    "          'Wide hallways', 'Safety card', 'Bathtub', 'BBQ grill', 'Room-darkening shades', \n",
    "          'Well-lit path to entrance', 'Lake access', 'Ethernet connection', 'Wide entrance for guests', \n",
    "          'Flat path to guest entrance', 'Extra space around bed', 'Wide entrance']\n",
    "\n",
    "#obtain list of files\n",
    "name = get_file_names(root+'original_dataset')\n",
    "accum_kept = 0\n",
    "accum_removed = 0\n",
    "for i in name:\n",
    "    print(\"Processing: \", i)\n",
    "    filepath = root+'full_dataset/'+i\n",
    "    df = initialization(filepath)\n",
    "    start_len = len(df)\n",
    "    print(\"--Start # of Samples:\", start_len)\n",
    "    \n",
    "    df = missing_data(df)\n",
    "    df = remove_outlier(df)\n",
    "    df = amenityRemoval(df, amenToKeep)\n",
    "    df = pd.get_dummies(df)\n",
    "    df = nomarlize_price(df)\n",
    "    #df = map_neighbourhoods(df, n_data)\n",
    "    #df = cat_to_code(df, root, categories)\n",
    "    #df = filter_categorical(df, properties, can_policy)\n",
    "    df = df.reset_index(drop=True)\n",
    "    end_len = len(df)\n",
    "    print(\"--End # of Samples:\", end_len)\n",
    "    print(\"--Total # of Samples Removed\", start_len-end_len)\n",
    "    save_file(root, i, df)\n",
    "    accum_kept += end_len\n",
    "    accum_removed += start_len-end_len\n",
    "    print(\"--Accumulated # of Samples:\", accum_kept)\n",
    "\n",
    "print(\"===========================================\")\n",
    "print(\"Data Processing Complete\")\n",
    "print(\"--Total # of Samples Processed: \", accum_kept+accum_removed)\n",
    "print(\"--Total # of Samples Kept: \", accum_kept, \"(\"+str(accum_kept/(accum_kept+accum_removed)*100)+\"%)\")\n",
    "print(\"--Total # of Samples Removed: \", accum_removed, \"(\"+str(accum_removed/(accum_kept+accum_removed)*100)+\"%)\")\n",
    "    \n",
    "#save_file(name[1],df)\n",
    "#df = missing_data(df)\n",
    "#df = remove_outlier(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root+\"processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
