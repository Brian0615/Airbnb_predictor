{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Req'd Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Getting File Names & Saving csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all the file_names in a given directory\n",
    "def get_file_names(folder):\n",
    "    #Listing entries present in given folder\n",
    "    entries = os.listdir(folder)\n",
    "    for i in entries:\n",
    "        if 'csv' not in i:\n",
    "            entries.remove(i)\n",
    "    return sorted(entries, reverse=True)\n",
    "\n",
    "#Saving the file\n",
    "def save_file(root, name_of_file, my_dataframe):\n",
    "    #Test if save directory exists\n",
    "    try:\n",
    "        my_dataframe.to_csv(root+'processed_data/'+ name_of_file, index=False)\n",
    "    #Otherwise make the directory and then save\n",
    "    except:\n",
    "        os.mkdir(root+'processed_data')\n",
    "        my_dataframe.to_csv(root+'processed_data/'+ name_of_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Initialization of File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(filepath):\n",
    "    df = pd.read_csv(filepath, \n",
    "                     usecols = ['id','last_scraped', 'host_is_superhost', \n",
    "                            'latitude','longitude', 'property_type',\n",
    "                           'room_type','accommodates','bathrooms',\n",
    "                           'bedrooms','beds','amenities', 'price',\n",
    "                            'instant_bookable','cancellation_policy'])\n",
    "    df[\"last_scraped\"] = pd.to_datetime(df[\"last_scraped\"])\n",
    "    df[\"price\"] = df[\"price\"].apply(lambda x: x.replace('$','').replace(',', '').replace('.00', '')).astype(\"int\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Removing Missing Data and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove missing data\n",
    "def missing_data(my_dataframe):\n",
    "    #computing number of total rows\n",
    "    total_rows = my_dataframe.shape[0]\n",
    "    #Dropping rows if empty cells are present in any of the given rows\n",
    "    my_dataframe = my_dataframe.dropna(subset = ['id','last_scraped', 'host_is_superhost', \n",
    "                            'latitude','longitude', 'property_type',\n",
    "                           'room_type','accommodates','bathrooms',\n",
    "                           'bedrooms','beds','amenities', 'price',\n",
    "                            'instant_bookable','cancellation_policy'])\n",
    "    #Computing number of rows left after removing rows for error checking\n",
    "    total_rows_after = my_dataframe.shape[0]\n",
    "    \n",
    "    #Printing\n",
    "    print('total rows before missing data removal:', total_rows)\n",
    "    print('total rows after missing data removal:', total_rows_after)\n",
    "    \n",
    "    return my_dataframe\n",
    "\n",
    "\n",
    "def remove_outlier(my_dataframe):\n",
    "    \n",
    "    #Plotting a box and whisker plot for data visualization\n",
    "    #sns.boxplot(x=my_dataframe['price'])\n",
    "    #Computing the quantiles of the box and whisker plot\n",
    "    quantiles = my_dataframe['price'].quantile([0.0001, 0.01,0.05, 0.25,0.5,0.75,0.90, 0.95,0.99])\n",
    "    \n",
    "    #computing number of total rows prior to removing outliers\n",
    "    total_rows = my_dataframe.shape[0]\n",
    "    print('Number of rows before outlier removal',total_rows)\n",
    "    \n",
    "    #Adjusting data based on quantile information\n",
    "    my_new_dataframe = my_dataframe[(my_dataframe['price'] < quantiles.loc[0.95]) & (my_dataframe['price'] > quantiles.loc[0.05])]\n",
    "    quantiles = my_new_dataframe['price'].quantile([0.0001, 0.01,0.05, 0.25,0.5,0.75,0.90, 0.95,0.99])\n",
    "    sns.boxplot(x=my_new_dataframe['price'])\n",
    "    \n",
    "    #computing new number of total rows\n",
    "    total_rows = my_new_dataframe.shape[0]\n",
    "    \n",
    "    print('Number of rows after outlier removal',total_rows)\n",
    "\n",
    "    return my_new_dataframe\n",
    "\n",
    "#Function for normalizing data \n",
    "def nomarlize_price(my_dataframe):\n",
    "    my_dataframe['price'] = (my_dataframe['price'] - \n",
    "                             my_dataframe['price'].min())/(my_dataframe['price'].max() - \n",
    "                                                         my_dataframe['price'].min())\n",
    "    return my_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Mapping Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map neighbourhood names to modified MLS district codes\n",
    "def map_neighbourhoods(data, n_data):\n",
    "    \n",
    "    district = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district\"].tolist()))\n",
    "    district_group = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district_group\"].tolist()))\n",
    "    district_number = dict(zip(n_data[\"neighbourhood\"].tolist(), n_data[\"district_number\"].tolist()))\n",
    "\n",
    "    \n",
    "    data[\"district_group\"] = data[\"neighbourhood_cleansed\"].map(district_group)\n",
    "    data[\"district_number\"] = data[\"neighbourhood_cleansed\"].map(district_number)\n",
    "    data[\"district\"] = data[\"district_group\"]*15+data[\"district_number\"]\n",
    "    return data\n",
    "\n",
    "# Convert categorical data (e.g. host_is_superhost) into numerical codes\n",
    "def cat_to_code(data, root, categories):\n",
    "    category_codes = {}\n",
    "    \n",
    "    for i in categories:\n",
    "        name = data[i].name\n",
    "        category_codes[name] = data[i].astype(\"category\").cat.categories # save list of categories\n",
    "        data[name+\"_codes\"] = data[i].astype(\"category\").cat.codes # convert to code\n",
    "    \n",
    "    for i in category_codes:\n",
    "        df = pd.DataFrame()\n",
    "        df[i] = category_codes[i]\n",
    "        df.to_csv(root+r\"category_codes/\"+i+\".csv\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Filter out categories with a small percentage (<5% of data)\n",
    "def filter_categorical(data, properties, can_policy):\n",
    "    num_samples = data.shape[0]\n",
    "    data = data[data.property_type.isin(properties)]\n",
    "    num_samples2 = data.shape[0]\n",
    "    data = data[data.cancellation_policy.isin(can_policy)]\n",
    "    num_samples3 = data.shape[0]\n",
    "    removed = {\"properties\": num_samples-num_samples2,\n",
    "               \"can_policy\": num_samples2-num_samples3}\n",
    "    print(\"Number removed:\")\n",
    "    print(removed)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Filtering of Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amenityRemoval(df, toKeep):\n",
    "    toKeep = ['TV', 'Internet', 'Wifi', 'Air conditioning', 'Kitchen', 'Indoor fireplace', 'Heating', \n",
    "          'Family/kid friendly', 'Washer', 'Dryer', 'Smoke detector', 'First aid kit', 'Fire extinguisher', \n",
    "          'Essentials', 'Shampoo', 'Cable TV', 'Pool', 'Free parking on premises', 'Doorman', 'Gym', 'Elevator', \n",
    "          'Buzzer/wireless intercom', 'Free street parking', 'Carbon monoxide detector', 'Lock on bedroom door', \n",
    "          '24-hour check-in', 'Hangers', 'Hair dryer', 'Iron', 'Laptop friendly workspace',  \n",
    "          'Self check-in', 'Keypad', 'Private entrance', 'Hot water', 'Bed linens', 'Extra pillows and blankets', \n",
    "          'Microwave', 'Coffee maker', 'Refrigerator', 'Dishes and silverware', 'Cooking basics', 'Oven', 'Stove', \n",
    "          'Long term stays allowed', 'No stairs or steps to enter', 'Wheelchair accessible', 'Pets allowed', \n",
    "          'Dishwasher', 'Single level home', 'Patio or balcony', 'Luggage dropoff allowed', \n",
    "          'Paid parking on premises', 'Host greets you', 'Lockbox', 'Garden or backyard', \n",
    "          'Paid parking off premises', 'Private living room', 'Breakfast', 'Hot tub', \n",
    "          'Wide hallways', 'Safety card', 'Bathtub', 'BBQ grill', 'Room-darkening shades', \n",
    "          'Well-lit path to entrance', 'Lake access', 'Ethernet connection', 'Wide entrance for guests', \n",
    "          'Flat path to guest entrance', 'Extra space around bed', 'Wide entrance']\n",
    "    amen = []\n",
    "    \n",
    "    for p in range(len(df)):\n",
    "        amen = df[\"amenities\"][p]\n",
    "        amen = amen[1:len(amen)-1]\n",
    "        amen = str(amen)\n",
    "        amen = amen.replace(\"\\\"\", \"\")\n",
    "        amen = amen.split(\",\")\n",
    "\n",
    "    for i in amen:\n",
    "        if i not in toKeep:\n",
    "            amen.remove(i)   \n",
    "\n",
    "    for j in toKeep:\n",
    "        df[\"amenities_\"+j] = df[\"amenities\"].apply(lambda x: j in x).astype(int)\n",
    "    \n",
    "    df = df.drop(columns=\"amenities\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  19_08_08_listings.csv\n",
      "--Start # of Samples: 21617\n",
      "total rows before missing data removal: 21617\n",
      "total rows after missing data removal: 21580\n",
      "Number of rows before outlier removal 21580\n",
      "Number of rows after outlier removal 19247\n",
      "--End # of Samples: 19247\n",
      "--Total # of Samples Removed 2370\n",
      "--Accumulated # of Samples: 19247\n",
      "Processing:  19_07_08_listings.csv\n",
      "--Start # of Samples: 21312\n",
      "total rows before missing data removal: 21312\n",
      "total rows after missing data removal: 21265\n",
      "Number of rows before outlier removal 21265\n",
      "Number of rows after outlier removal 18954\n",
      "--End # of Samples: 18954\n",
      "--Total # of Samples Removed 2358\n",
      "--Accumulated # of Samples: 38201\n",
      "Processing:  19_06_04_listings.csv\n",
      "--Start # of Samples: 20769\n",
      "total rows before missing data removal: 20769\n",
      "total rows after missing data removal: 20724\n",
      "Number of rows before outlier removal 20724\n",
      "Number of rows after outlier removal 18429\n",
      "--End # of Samples: 18429\n",
      "--Total # of Samples Removed 2340\n",
      "--Accumulated # of Samples: 56630\n",
      "Processing:  19_05_06_listings.csv\n",
      "--Start # of Samples: 20303\n",
      "total rows before missing data removal: 20303\n",
      "total rows after missing data removal: 20244\n",
      "Number of rows before outlier removal 20244\n",
      "Number of rows after outlier removal 18048\n",
      "--End # of Samples: 18048\n",
      "--Total # of Samples Removed 2255\n",
      "--Accumulated # of Samples: 74678\n",
      "Processing:  19_04_08_listings.csv\n",
      "--Start # of Samples: 20143\n",
      "total rows before missing data removal: 20143\n",
      "total rows after missing data removal: 20094\n",
      "Number of rows before outlier removal 20094\n",
      "Number of rows after outlier removal 17845\n",
      "--End # of Samples: 17845\n",
      "--Total # of Samples Removed 2298\n",
      "--Accumulated # of Samples: 92523\n",
      "Processing:  19_03_07_listings.csv\n",
      "--Start # of Samples: 20036\n",
      "total rows before missing data removal: 20036\n",
      "total rows after missing data removal: 19990\n",
      "Number of rows before outlier removal 19990\n",
      "Number of rows after outlier removal 17684\n",
      "--End # of Samples: 17684\n",
      "--Total # of Samples Removed 2352\n",
      "--Accumulated # of Samples: 110207\n",
      "Processing:  19_02_04_listings.csv\n",
      "--Start # of Samples: 19885\n",
      "total rows before missing data removal: 19885\n",
      "total rows after missing data removal: 19846\n",
      "Number of rows before outlier removal 19846\n",
      "Number of rows after outlier removal 17526\n",
      "--End # of Samples: 17526\n",
      "--Total # of Samples Removed 2359\n",
      "--Accumulated # of Samples: 127733\n",
      "Processing:  19_01_13_listings.csv\n",
      "--Start # of Samples: 19653\n",
      "total rows before missing data removal: 19653\n",
      "total rows after missing data removal: 19615\n",
      "Number of rows before outlier removal 19615\n",
      "Number of rows after outlier removal 17417\n",
      "--End # of Samples: 17417\n",
      "--Total # of Samples Removed 2236\n",
      "--Accumulated # of Samples: 145150\n",
      "Processing:  18_12_06_listings.csv\n",
      "--Start # of Samples: 19255\n",
      "total rows before missing data removal: 19255\n",
      "total rows after missing data removal: 19219\n",
      "Number of rows before outlier removal 19219\n",
      "Number of rows after outlier removal 17052\n",
      "--End # of Samples: 17052\n",
      "--Total # of Samples Removed 2203\n",
      "--Accumulated # of Samples: 162202\n",
      "Processing:  18_11_04_listings.csv\n",
      "--Start # of Samples: 18509\n",
      "total rows before missing data removal: 18509\n",
      "total rows after missing data removal: 18470\n",
      "Number of rows before outlier removal 18470\n",
      "Number of rows after outlier removal 16341\n",
      "--End # of Samples: 16341\n",
      "--Total # of Samples Removed 2168\n",
      "--Accumulated # of Samples: 178543\n",
      "Processing:  18_10_06_listings.csv\n",
      "--Start # of Samples: 17343\n",
      "total rows before missing data removal: 17343\n",
      "total rows after missing data removal: 17258\n",
      "Number of rows before outlier removal 17258\n",
      "Number of rows after outlier removal 15361\n",
      "--End # of Samples: 15361\n",
      "--Total # of Samples Removed 1982\n",
      "--Accumulated # of Samples: 193904\n",
      "Processing:  18_09_08_listings.csv\n",
      "--Start # of Samples: 17682\n",
      "total rows before missing data removal: 17682\n",
      "total rows after missing data removal: 17643\n",
      "Number of rows before outlier removal 17643\n",
      "Number of rows after outlier removal 15636\n",
      "--End # of Samples: 15636\n",
      "--Total # of Samples Removed 2046\n",
      "--Accumulated # of Samples: 209540\n",
      "===========================================\n",
      "Data Processing Complete\n",
      "--Total # of Samples Processed:  236507\n",
      "--Total # of Samples Kept:  209540 (88.59780048793482%)\n",
      "--Total # of Samples Removed:  26967 (11.402199512065183%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get list of filenames\n",
    "root = '../'\n",
    "\n",
    "# Categories to convert to code\n",
    "categories = [\"host_is_superhost\", \"property_type\", \"room_type\", \"instant_bookable\", \"cancellation_policy\"]\n",
    "\n",
    "#Properties and cancellation policies to filter\n",
    "properties = [\"Apartment\", \"Condominium\", \"House\", \"Townhouse\", \"Guest suite\", \"Bungalow\"]\n",
    "can_policy = [\"flexible\", \"moderate\", \"strict_14_with_grace_period\"]\n",
    "\n",
    "#mapping for neighbourhoods\n",
    "n_data = pd.read_csv(root+'category_codes/neighbourhoods.csv')\n",
    "\n",
    "#List of Amenities to Keep -> has more than 5%\n",
    "amenToKeep = ['TV', 'Internet', 'Wifi', 'Air conditioning', 'Kitchen', 'Indoor fireplace', 'Heating', \n",
    "          'Family/kid friendly', 'Washer', 'Dryer', 'Smoke detector', 'First aid kit', 'Fire extinguisher', \n",
    "          'Essentials', 'Shampoo', 'Cable TV', 'Pool', 'Free parking on premises', 'Doorman', 'Gym', 'Elevator', \n",
    "          'Buzzer/wireless intercom', 'Free street parking', 'Carbon monoxide detector', 'Lock on bedroom door', \n",
    "          '24-hour check-in', 'Hangers', 'Hair dryer', 'Iron', 'Laptop friendly workspace',  \n",
    "          'Self check-in', 'Keypad', 'Private entrance', 'Hot water', 'Bed linens', 'Extra pillows and blankets', \n",
    "          'Microwave', 'Coffee maker', 'Refrigerator', 'Dishes and silverware', 'Cooking basics', 'Oven', 'Stove', \n",
    "          'Long term stays allowed', 'No stairs or steps to enter', 'Wheelchair accessible', 'Pets allowed', \n",
    "          'Dishwasher', 'Single level home', 'Patio or balcony', 'Luggage dropoff allowed', \n",
    "          'Paid parking on premises', 'Host greets you', 'Lockbox', 'Garden or backyard', \n",
    "          'Paid parking off premises', 'Private living room', 'Breakfast', 'Hot tub', \n",
    "          'Wide hallways', 'Safety card', 'Bathtub', 'BBQ grill', 'Room-darkening shades', \n",
    "          'Well-lit path to entrance', 'Lake access', 'Ethernet connection', 'Wide entrance for guests', \n",
    "          'Flat path to guest entrance', 'Extra space around bed', 'Wide entrance']\n",
    "\n",
    "#obtain list of files\n",
    "name = get_file_names(root+'original_dataset')\n",
    "accum_kept = 0\n",
    "accum_removed = 0\n",
    "for i in name:\n",
    "    print(\"Processing: \", i)\n",
    "    filepath = root+'original_dataset/'+i\n",
    "    df = initialization(filepath)\n",
    "    start_len = len(df)\n",
    "    print(\"--Start # of Samples:\", start_len)\n",
    "    \n",
    "    df = missing_data(df)\n",
    "    df = remove_outlier(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = amenityRemoval(df, amenToKeep)\n",
    "    #df = df.drop(\"amenities\")\n",
    "    df = pd.get_dummies(df)\n",
    "    #df = nomarlize_price(df)\n",
    "    #df = map_neighbourhoods(df, n_data)\n",
    "    #df = cat_to_code(df, root, categories)\n",
    "    #df = filter_categorical(df, properties, can_policy)\n",
    "    end_len = len(df)\n",
    "    print(\"--End # of Samples:\", end_len)\n",
    "    print(\"--Total # of Samples Removed\", start_len-end_len)\n",
    "    save_file(root, i, df)\n",
    "    accum_kept += end_len\n",
    "    accum_removed += start_len-end_len\n",
    "    print(\"--Accumulated # of Samples:\", accum_kept)\n",
    "\n",
    "print(\"===========================================\")\n",
    "print(\"Data Processing Complete\")\n",
    "print(\"--Total # of Samples Processed: \", accum_kept+accum_removed)\n",
    "print(\"--Total # of Samples Kept: \", accum_kept, \"(\"+str(accum_kept/(accum_kept+accum_removed)*100)+\"%)\")\n",
    "print(\"--Total # of Samples Removed: \", accum_removed, \"(\"+str(accum_removed/(accum_kept+accum_removed)*100)+\"%)\")\n",
    "    \n",
    "#save_file(name[1],df)\n",
    "#df = missing_data(df)\n",
    "#df = remove_outlier(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../processed_data/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root+\"processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17248, 263)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
